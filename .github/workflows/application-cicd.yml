name: "Bring Up/Down App"

on:
  push:
    branches:
      - main
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'k8s/**'
      - 'terraform/**'
      - .github/workflows/application-cicd.yml
  pull_request:
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'k8s/**'
      - 'terraform/**'
      - .github/workflows/application-cicd.yml
    branches:
      - main
  workflow_dispatch:
    inputs:
      action:
        description: 'Terraform Action to perform'
        required: true
        default: 'apply'
        type: choice
        options:
          - apply
          - destroy

env:
  PROJECT_NAME: "best-qr-ever"
  AWS_REGION: "us-east-1"

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  terraform:
    name: "Infrastructure"
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./terraform
    outputs:
      deployer_role_arn: ${{ steps.set-outputs.outputs.deployer_role_arn }}
      cluster_name: ${{ steps.set-outputs.outputs.cluster_name }}
      tf_outcome: ${{ steps.apply.outcome }}
      alb_security_group_id: ${{ steps.set-outputs.outputs.alb_security_group_id }}
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::176971015975:role/GitHub_Actions_Resources_Deployment
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.9.0"

      - name: Terraform Init
        run: terraform init -upgrade

      - name: Terraform Format & Validate
        run: |
          terraform fmt -check
          terraform validate

      - name: Terraform Plan
        id: plan
        if: github.event_name == 'pull_request' || github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && inputs.action == 'apply')
        run: terraform plan -no-color -out=tfplan

      - name: Update Pull Request with Plan
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        env:
          PLAN: "terraform\n${{ steps.plan.outputs.stdout }}"
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### Terraform Plan ðŸ“– \`${{ steps.plan.outcome }}\`
            <details><summary>Show Plan Output</summary>

            \`\`\`${process.env.PLAN}\`\`\`

            </details>

            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })

      - name: Terraform Plan Status
        if: steps.plan.outcome == 'failure'
        run: exit 1

      - name: Terraform Apply
        id: apply
        if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && inputs.action == 'apply')
        run: terraform apply -auto-approve tfplan

      - name: Export Terraform Outputs
        id: set-outputs
        if: steps.apply.outcome == 'success' || github.event.inputs.action != 'destroy'
        run: |
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "deployer_role_arn=$(terraform output -raw deployer_role_arn)" >> $GITHUB_OUTPUT
          echo "alb_security_group_id=$(terraform output -raw alb_security_group_id)" >> $GITHUB_OUTPUT

      - name: Kubernetes Cleanup (Pre-Destroy)
        if: github.event_name == 'workflow_dispatch' && inputs.action == 'destroy'
        run: |
          CLUSTER_NAME=$(terraform output -raw cluster_name || echo "none")
          if [ "$CLUSTER_NAME" != "none" ] && [ "$CLUSTER_NAME" != "" ]; then
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name $CLUSTER_NAME
            echo "Deleting Kubernetes resources that create AWS dependencies..."
            kubectl delete namespace qr-generator --timeout=180s || true
            echo "Waiting 2 minutes for AWS Load Balancer and ENI cleanup..."
            sleep 120
          else
            echo "Cluster not found or already gone. Proceeding to Terraform Destroy."
          fi

      - name: Terraform Destroy
        if: github.event_name == 'workflow_dispatch' && inputs.action == 'destroy'
        run: terraform destroy -auto-approve

  deploy:
    name: "App Deployment"
    needs: terraform
    runs-on: ubuntu-latest
    env:
      CLUSTER_NAME: ${{ needs.terraform.outputs.cluster_name }}
      DEPLOYER_ROLE_ARN: ${{ needs.terraform.outputs.deployer_role_arn }}
      ALB_SECURITY_GROUP_ID: ${{ needs.terraform.outputs.alb_security_group_id }}
    if: |
      always() && 
      github.event.inputs.action != 'destroy' && 
      (needs.terraform.result == 'success' || needs.terraform.result == 'skipped')

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.DEPLOYER_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and Push Docker Images
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/${{ env.PROJECT_NAME }}-backend:$IMAGE_TAG ./backend
          docker push $ECR_REGISTRY/${{ env.PROJECT_NAME }}-backend:$IMAGE_TAG

          docker build \
            --build-arg API_URL="http://qr-backend.qr-generator.svc.cluster.local:8000" \
            -t $ECR_REGISTRY/${{ env.PROJECT_NAME }}-frontend:$IMAGE_TAG ./frontend
          docker push $ECR_REGISTRY/${{ env.PROJECT_NAME }}-frontend:$IMAGE_TAG

          echo "BACKEND_IMAGE=$ECR_REGISTRY/${{ env.PROJECT_NAME }}-backend:$IMAGE_TAG" >> $GITHUB_ENV
          echo "FRONTEND_IMAGE=$ECR_REGISTRY/${{ env.PROJECT_NAME }}-frontend:$IMAGE_TAG" >> $GITHUB_ENV

      - name: Setup Kubeconfig
        run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Deploy to Kubernetes
        run: |
          export BACKEND_IMAGE=${{ steps.login-ecr.outputs.registry }}/${{ env.PROJECT_NAME }}-backend:${{ github.sha }}
          export FRONTEND_IMAGE=${{ steps.login-ecr.outputs.registry }}/${{ env.PROJECT_NAME }}-frontend:${{ github.sha }}

          kubectl apply -f k8s/00-namespace.yml
          kubectl apply -f k8s/config-env.yml
          kubectl apply -f k8s/01-rbac.yml

          envsubst < k8s/02-backend/deployment.yml | kubectl apply -f -
          kubectl apply -f k8s/02-backend/service.yml

          envsubst < k8s/03-frontend/deployment.yml | kubectl apply -f -
          kubectl apply -f k8s/03-frontend/service.yml

          envsubst < k8s/03-frontend/ingress.yml | kubectl apply -f -

          kubectl apply -f k8s/04-monitoring/metrics-server.yml
          kubectl apply -f k8s/04-monitoring/hpa.yml

      - name: Verify Deployment
        run: |
          kubectl rollout status deployment/qr-backend -n qr-generator --timeout=120s
          kubectl rollout status deployment/qr-frontend -n qr-generator --timeout=120s

      - name: Get Application URL
        run: |
          echo "Waiting for ALB DNS name..."
          ALB_URL=""
          for i in {1..20}; do
            ALB_URL=$(kubectl get ingress qr-ingress -n qr-generator -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "")
            if [ -n "$ALB_URL" ]; then break; fi
            sleep 15
          done

          if [ -n "$ALB_URL" ]; then
            echo "URL: http://$ALB_URL"
          else
            echo "Timed out waiting for URL"
            exit 1
          fi